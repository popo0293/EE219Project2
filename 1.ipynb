{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics.cluster import homogeneity_score,completeness_score, adjusted_rand_score, adjusted_mutual_info_score\n",
    "\n",
    "'''\n",
    "try:\n",
    "    nltk.download(\"stopwords\")  # if the host does not have the package\n",
    "except (RuntimeError):\n",
    "    pass\n",
    "'''\n",
    "\n",
    "# globals\n",
    "MIN_DF = 3\n",
    "\n",
    "\n",
    "class SparseToDenseArray(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        if hasattr(X, 'toarray'):\n",
    "            return X.toarray()\n",
    "        return X\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "\n",
    "def stem_and_tokenize(doc):\n",
    "    exclude = set(string.punctuation)\n",
    "    no_punctuation = ''.join(ch for ch in doc if ch not in exclude)\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    tokens = tokenizer.tokenize(no_punctuation)\n",
    "    stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    return [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(sublinear_tf=True, smooth_idf=False, use_idf=True)\n",
    "\n",
    "\n",
    "def doTFIDF(data, mindf):\n",
    "    vectorizer = CountVectorizer(min_df=mindf, stop_words=ENGLISH_STOP_WORDS, tokenizer=stem_and_tokenize)\n",
    "    m = vectorizer.fit_transform(data)\n",
    "    m_train_tfidf = tfidf_transformer.fit_transform(m)\n",
    "    return m_train_tfidf\n",
    "\n",
    "\n",
    "def test_stem_count_vectorize():\n",
    "    test_string = [\"Hello, Google. But I can't answer this call go going goes bowl bowls bowled!\"]\n",
    "    vectorizer = CountVectorizer(stop_words=ENGLISH_STOP_WORDS, tokenizer=stem_and_tokenize)\n",
    "    X = vectorizer.fit_transform(test_string)\n",
    "    feature_name = vectorizer.get_feature_names()\n",
    "    print(feature_name)\n",
    "    print(X.toarray())\n",
    "\n",
    "\n",
    "def analyze(label, prob, predict, classes, n):\n",
    "    if n <= 2:\n",
    "        fpr, tpr, thresholds = roc_curve(label, prob)\n",
    "        roc_auc = auc(fpr,tpr)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, color='lightsteelblue',\n",
    "                 lw=2, label='AUC (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='deeppink', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "    cmatrix = confusion_matrix(label, predict)\n",
    "    plt.imshow(cmatrix, interpolation='nearest', cmap=plt.cm.BuGn)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    # plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=25)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = 'd'\n",
    "    thresh = cmatrix.max() / 2.\n",
    "    for i, j in itertools.product(range(n), range(n)):\n",
    "        plt.text(j, i, format(cmatrix[i, j], fmt),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cmatrix[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"accuracy: \", accuracy_score(label, predict))\n",
    "    if n <= 2:\n",
    "        print(\"recall: \", recall_score(label, predict))\n",
    "        print(\"precision: \", precision_score(label, predict))\n",
    "    else:\n",
    "        print(\"recall: \", recall_score(label, predict, average='weighted'))\n",
    "        print(\"precision: \", precision_score(label, predict, average='weighted'))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "comp_tech_subclasses = ['comp.graphics', \n",
    "                        'comp.os.ms-windows.misc', \n",
    "                        'comp.sys.ibm.pc.hardware', \n",
    "                        'comp.sys.mac.hardware']\n",
    "                        \n",
    "rec_act_subclasses = ['rec.autos', \n",
    "                      'rec.motorcycles', \n",
    "                      'rec.sport.baseball', \n",
    "                      'rec.sport.hockey']\n",
    "train_data = fetch_20newsgroups(subset='train', categories=comp_tech_subclasses+rec_act_subclasses, shuffle=True, random_state=42)\n",
    "test_data = fetch_20newsgroups(subset='test', categories=comp_tech_subclasses+rec_act_subclasses, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With min_df = 3 , (training documents, terms extracted):  (4732, 14381)\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "logging.info(\"Problem a\")\n",
    "start = timer()\n",
    "X_train_tfidf = doTFIDF(train_data.data, MIN_DF)\n",
    "print(\"With min_df = %d , (training documents, terms extracted): \" % MIN_DF, X_train_tfidf.shape)\n",
    "\n",
    "\n",
    "\n",
    "duration = timer() - start\n",
    "logging.debug(\"Computation Time in secs: %d\" % duration)\n",
    "logging.info(\"finished Problem b\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
